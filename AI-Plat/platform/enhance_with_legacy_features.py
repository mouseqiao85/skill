"""
åŸºäºä¸Šä¸€ä»£åŠŸèƒ½ç‚¹åˆ†æç»“æœï¼Œå¢å¼ºAI-Platå¹³å°
"""

import json
from typing import Dict, List, Any
import os


def load_legacy_analysis():
    """åŠ è½½ä¸Šä¸€ä»£åŠŸèƒ½åˆ†æç»“æœ"""
    with open('legacy_features_analysis.json', 'r', encoding='utf-8') as f:
        return json.load(f)


def enhance_ontology_module(legacy_data: Dict[str, Any]):
    """å¢å¼ºæœ¬ä½“è®ºæ¨¡å—ï¼Œå€Ÿé‰´ä¸Šä¸€ä»£æ¨¡å‹ç®¡ç†åŠŸèƒ½"""
    print("[ENHANCE] å¢å¼ºæœ¬ä½“è®ºæ¨¡å—...")
    
    # åˆ†ææ¨¡å‹ç›¸å…³åŠŸèƒ½ï¼Œç”¨äºæ”¹è¿›æœ¬ä½“è®ºè®¾è®¡
    model_management_features = []
    for feature in legacy_data['valuable_features']:
        if any(keyword in str(feature).lower() for keyword in ['model', 'æ¨¡å‹', 'asset', 'èµ„äº§']):
            model_management_features.append(feature)
    
    print(f"   è¯†åˆ«åˆ° {len(model_management_features)} ä¸ªæ¨¡å‹ç®¡ç†ç›¸å…³åŠŸèƒ½")
    
    # åˆ›å»ºæ¨¡å‹æœ¬ä½“å®šä¹‰ç¤ºä¾‹
    model_ontology_example = """
# æ¨¡å‹èµ„äº§ç®¡ç†æœ¬ä½“å®šä¹‰ç¤ºä¾‹
@prefix mmo: <http://ai-plat.org/model-management-ontology#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

mmo:AIModel a owl:Class ;
    rdfs:label "AIæ¨¡å‹" ;
    rdfs:comment "äººå·¥æ™ºèƒ½æ¨¡å‹çš„é€šç”¨è¡¨ç¤º" .

mmo:ModelVersion a owl:Class ;
    rdfs:label "æ¨¡å‹ç‰ˆæœ¬" ;
    rdfs:comment "AIæ¨¡å‹çš„ç‰¹å®šç‰ˆæœ¬" .

mmo:modelFramework a owl:ObjectProperty ;
    rdfs:label "æ¨¡å‹æ¡†æ¶" ;
    rdfs:domain mmo:AIModel ;
    rdfs:range mmo:Framework .

mmo:modelType a owl:ObjectProperty ;
    rdfs:label "æ¨¡å‹ç±»å‹" ;
    rdfs:domain mmo:AIModel ;
    rdfs:range mmo:ModelType .

mmo:trainingMethod a owl:ObjectProperty ;
    rdfs:label "è®­ç»ƒæ–¹æ³•" ;
    rdfs:domain mmo:AIModel ;
    rdfs:range mmo:TrainingMethod .

# æ¨¡å‹ç±»å‹æšä¸¾
mmo:GenerativeAI a mmo:ModelType ; rdfs:label "ç”Ÿæˆå¼AI" .
mmo:DiscriminativeAI a mmo:ModelType ; rdfs:label "åˆ¤åˆ«å¼AI" .
mmo:LargeLanguageModel a mmo:ModelType ; rdfs:label "å¤§è¯­è¨€æ¨¡å‹" .

# è®­ç»ƒæ–¹æ³•æšä¸¾
mmo:FullTuning a mmo:TrainingMethod ; rdfs:label "å…¨é‡æ›´æ–°" .
mmo:LoRA a mmo:TrainingMethod ; rdfs:label "LoRA" .
mmo:SFT a mmo:TrainingMethod ; rdfs:label "SFT" .
mmo:DPO a mmo:TrainingMethod ; rdfs:label "DPO" .
    """
    
    # åˆ›å»ºæ¨¡å‹æœ¬ä½“å®šä¹‰æ–‡ä»¶
    ontology_dir = "ontology/definitions"
    os.makedirs(ontology_dir, exist_ok=True)
    
    with open(f"{ontology_dir}/model_asset_ontology.ttl", "w", encoding="utf-8") as f:
        f.write(model_ontology_example)
    
    print("   âœ“ åˆ›å»ºæ¨¡å‹èµ„äº§ç®¡ç†æœ¬ä½“å®šä¹‰")


def enhance_agent_module(legacy_data: Dict[str, Any]):
    """å¢å¼ºæ™ºèƒ½ä½“æ¨¡å—ï¼Œå€Ÿé‰´ä¸Šä¸€ä»£è®­ç»ƒå’Œæ¨ç†åŠŸèƒ½"""
    print("[ENHANCE] å¢å¼ºæ™ºèƒ½ä½“æ¨¡å—...")
    
    # åˆ†æä»»åŠ¡å’Œä½œä¸šç›¸å…³åŠŸèƒ½
    task_features = []
    for feature in legacy_data['valuable_features']:
        if any(keyword in str(feature).lower() for keyword in ['task', 'job', 'è®­ç»ƒ', 'ä½œä¸š', 'æ¨ç†']):
            task_features.append(feature)
    
    print(f"   è¯†åˆ«åˆ° {len(task_features)} ä¸ªä»»åŠ¡/ä½œä¸šç›¸å…³åŠŸèƒ½")
    
    # åˆ›å»ºç¤ºä¾‹æŠ€èƒ½ - æ¨¡å‹è®­ç»ƒæŠ€èƒ½
    training_skill_example = '''
"""
æ¨¡å‹è®­ç»ƒæŠ€èƒ½
åŸºäºä¸Šä¸€ä»£å¹³å°çš„è®­ç»ƒåŠŸèƒ½å®ç°
"""

from agents.skill_registry import global_skill_registry, SkillCategory
from typing import Dict, Any, List


@global_skill_registry.register_skill(
    name="model_training",
    description="æ‰§è¡ŒAIæ¨¡å‹è®­ç»ƒä»»åŠ¡",
    version="1.0.0",
    author="AI-Plat Team",
    category=SkillCategory.ML_MODEL,
    tags=["training", "ml", "ai", "model"]
)
def model_training(
    model_type: str,
    training_method: str = "fine_tuning",
    dataset_path: str = "",
    hyperparameters: Dict[str, Any] = None,
    resources: Dict[str, Any] = None
) -> Dict[str, Any]:
    """
    æ‰§è¡Œæ¨¡å‹è®­ç»ƒä»»åŠ¡
    
    Args:
        model_type: æ¨¡å‹ç±»å‹ (e.g., "large_language_model", "vision_model")
        training_method: è®­ç»ƒæ–¹æ³• ("full_tuning", "lora", "sft", "dpo")
        dataset_path: è®­ç»ƒæ•°æ®é›†è·¯å¾„
        hyperparameters: è¶…å‚æ•°é…ç½®
        resources: èµ„æºé…ç½® (CPU, GPU, memoryç­‰)
    
    Returns:
        è®­ç»ƒç»“æœ
    """
    if hyperparameters is None:
        hyperparameters = {}
    
    if resources is None:
        resources = {
            "cpu_cores": 8,
            "gpu_type": "nvidia",
            "gpu_count": 1,
            "memory_gb": 32
        }
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    result = {
        "status": "completed",
        "model_type": model_type,
        "training_method": training_method,
        "dataset": dataset_path,
        "hyperparameters_used": hyperparameters,
        "resources_allocated": resources,
        "estimated_duration": "2h 30m",
        "metrics": {
            "final_loss": 0.15,
            "accuracy": 0.92,
            "convergence_rate": 0.98
        }
    }
    
    print(f"æ¨¡å‹è®­ç»ƒä»»åŠ¡å®Œæˆ: {model_type} using {training_method}")
    return result


@global_skill_registry.register_skill(
    name="model_evaluation",
    description="è¯„ä¼°AIæ¨¡å‹æ€§èƒ½",
    version="1.0.0",
    author="AI-Plat Team", 
    category=SkillCategory.ML_MODEL,
    tags=["evaluation", "assessment", "ml", "ai"]
)
def model_evaluation(
    model_id: str,
    evaluation_type: str = "automatic",
    test_dataset: str = "",
    evaluation_metrics: List[str] = None
) -> Dict[str, Any]:
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    Args:
        model_id: æ¨¡å‹ID
        evaluation_type: è¯„ä¼°ç±»å‹ ("automatic", "baseline", "human_judgment")
        test_dataset: æµ‹è¯•æ•°æ®é›†
        evaluation_metrics: è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨
    
    Returns:
        è¯„ä¼°ç»“æœ
    """
    if evaluation_metrics is None:
        evaluation_metrics = ["accuracy", "f1_score", "precision", "recall"]
    
    # æ¨¡æ‹Ÿè¯„ä¼°è¿‡ç¨‹
    result = {
        "model_id": model_id,
        "evaluation_type": evaluation_type,
        "test_dataset": test_dataset,
        "metrics": {
            "accuracy": 0.94,
            "f1_score": 0.92,
            "precision": 0.95,
            "recall": 0.90,
            "bleu_score": 0.85 if "bleu" in evaluation_metrics else None,
            "rouge_scores": {
                "rouge_1": 0.78,
                "rouge_2": 0.65,
                "rouge_l": 0.72
            } if any("rouge" in metric.lower() for metric in evaluation_metrics) else None
        },
        "report_url": f"/reports/evaluation_{model_id}.html",
        "passed": True
    }
    
    print(f"æ¨¡å‹è¯„ä¼°å®Œæˆ: {model_id}")
    return result


@global_skill_registry.register_skill(
    name="model_inference",
    description="æ‰§è¡Œæ¨¡å‹æ¨ç†æœåŠ¡",
    version="1.0.0",
    author="AI-Plat Team",
    category=SkillCategory.ML_MODEL,
    tags=["inference", "prediction", "ml", "ai", "deployment"]
)
def model_inference(
    model_id: str,
    input_data: Any,
    deployment_config: Dict[str, Any] = None
) -> Dict[str, Any]:
    """
    æ‰§è¡Œæ¨¡å‹æ¨ç†
    
    Args:
        model_id: æ¨¡å‹ID
        input_data: è¾“å…¥æ•°æ®
        deployment_config: éƒ¨ç½²é…ç½®
    
    Returns:
        æ¨ç†ç»“æœ
    """
    if deployment_config is None:
        deployment_config = {
            "batch_size": 1,
            "timeout": 30,
            "max_tokens": 2048
        }
    
    # æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹
    result = {
        "model_id": model_id,
        "input_processed": len(str(input_data)) if hasattr(input_data, '__len__') else 1,
        "inference_time_ms": 245,
        "output": "Simulated inference output based on input",
        "confidence": 0.96,
        "deployment_config_used": deployment_config
    }
    
    print(f"æ¨¡å‹æ¨ç†å®Œæˆ: {model_id}")
    return result
'''
    
    # åˆ›å»ºæŠ€èƒ½å®šä¹‰æ–‡ä»¶
    skills_dir = "agents/skills"
    os.makedirs(skills_dir, exist_ok=True)
    
    with open(f"{skills_dir}/model_operations.py", "w", encoding="utf-8") as f:
        f.write(training_skill_example)
    
    print("   âœ“ åˆ›å»ºæ¨¡å‹æ“ä½œç›¸å…³æŠ€èƒ½")


def enhance_vibecoding_module(legacy_data: Dict[str, Any]):
    """å¢å¼ºVibecodingæ¨¡å—ï¼Œå€Ÿé‰´ä¸Šä¸€ä»£å¼€å‘ä½“éªŒ"""
    print("[ENHANCE] å¢å¼ºVibecodingæ¨¡å—...")
    
    # åˆ†æå¼€å‘ç›¸å…³åŠŸèƒ½
    dev_features = []
    for feature in legacy_data['valuable_features']:
        if any(keyword in str(feature).lower() for keyword in ['notebook', 'code', 'dev', 'å¼€å‘', 'ç¼–ç¨‹']):
            dev_features.append(feature)
    
    print(f"   è¯†åˆ«åˆ° {len(dev_features)} ä¸ªå¼€å‘ç›¸å…³åŠŸèƒ½")
    
    # åˆ›å»ºç¤ºä¾‹ä»£ç ç”Ÿæˆæ¨¡æ¿
    code_templates = '''
"""
ä»£ç ç”Ÿæˆæ¨¡æ¿
åŸºäºä¸Šä¸€ä»£å¹³å°çš„å¼€å‘åŠŸèƒ½ç»éªŒ
"""

from vibecoding.code_generator import CodeGenerator
from typing import Dict, Any


def create_training_pipeline_template() -> str:
    """
    åˆ›å»ºè®­ç»ƒæµæ°´çº¿ä»£ç æ¨¡æ¿
    å‚è€ƒä¸Šä¸€ä»£å¹³å°çš„Notebookå»ºæ¨¡å’Œä½œä¸šå»ºæ¨¡åŠŸèƒ½
    """
    template = """
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import os

class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
    acc = accuracy_score(labels, predictions)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

def train_model(model_name, train_texts, train_labels, val_texts, val_labels, output_dir="./model_output"):
    # Initialize tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=len(set(train_labels))  # Adjust based on number of unique labels
    )
    
    # Create datasets
    train_dataset = TextDataset(train_texts, train_labels, tokenizer)
    val_dataset = TextDataset(val_texts, val_labels, tokenizer)
    
    # Define training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
        logging_steps=10,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
    )
    
    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
    )
    
    # Train the model
    trainer.train()
    
    # Save the model
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    return trainer

# Example usage
if __name__ == "__main__":
    # Load your data
    # train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)
    
    # train_model("bert-base-uncased", train_texts, train_labels, val_texts, val_labels)
    print("Training pipeline template created successfully!")
"""
    return template


def create_evaluation_script_template() -> str:
    """
    åˆ›å»ºè¯„ä¼°è„šæœ¬ä»£ç æ¨¡æ¿
    å‚è€ƒä¸Šä¸€ä»£å¹³å°çš„æ¨¡å‹è¯„ä¼°åŠŸèƒ½
    """
    template = """
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any
import json
from datetime import datetime

class ModelEvaluator:
    def __init__(self, model_name: str, task_type: str = "classification"):
        """
        åˆå§‹åŒ–æ¨¡å‹è¯„ä¼°å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            task_type: ä»»åŠ¡ç±»å‹ ("classification", "regression", "generation")
        """
        self.model_name = model_name
        self.task_type = task_type
        self.evaluation_results = {}
        self.timestamp = datetime.now().isoformat()

    def evaluate_classification(self, y_true: List, y_pred: List, y_pred_proba: List = None) -> Dict[str, Any]:
        """
        è¯„ä¼°åˆ†ç±»æ¨¡å‹
        
        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_pred: é¢„æµ‹æ ‡ç­¾
            y_pred_proba: é¢„æµ‹æ¦‚ç‡ (å¯é€‰)
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
        
        results = {
            "model_name": self.model_name,
            "task_type": "classification",
            "timestamp": self.timestamp,
            "metrics": {
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall, 
                "f1_score": f1
            },
            "samples_count": len(y_true)
        }
        
        # å¦‚æœæä¾›äº†æ¦‚ç‡ï¼Œè®¡ç®—AUC
        if y_pred_proba is not None:
            try:
                auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr')
                results["metrics"]["auc"] = auc
            except:
                pass  # AUCä¸å¯ç”¨æ—¶å¿½ç•¥
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        results["confusion_matrix"] = cm.tolist()
        
        self.evaluation_results = results
        return results

    def evaluate_regression(self, y_true: List, y_pred: List) -> Dict[str, Any]:
        """
        è¯„ä¼°å›å½’æ¨¡å‹
        """
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        results = {
            "model_name": self.model_name,
            "task_type": "regression",
            "timestamp": self.timestamp,
            "metrics": {
                "mse": mse,
                "rmse": rmse,
                "mae": mae,
                "r2_score": r2
            },
            "samples_count": len(y_true)
        }
        
        self.evaluation_results = results
        return results

    def evaluate_generation(self, references: List[str], predictions: List[str]) -> Dict[str, Any]:
        """
        è¯„ä¼°ç”Ÿæˆæ¨¡å‹ (ä½¿ç”¨ç®€åŒ–çš„æŒ‡æ ‡)
        """
        # ç®€åŒ–çš„è¯„ä¼° - å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æŒ‡æ ‡
        exact_match = sum(1 for ref, pred in zip(references, predictions) if ref.strip() == pred.strip()) / len(references)
        
        # è®¡ç®—å¹³å‡é•¿åº¦å·®å¼‚
        ref_lengths = [len(ref.split()) for ref in references]
        pred_lengths = [len(pred.split()) for pred in predictions]
        avg_length_diff = np.mean([abs(r - p) for r, p in zip(ref_lengths, pred_lengths)])
        
        results = {
            "model_name": self.model_name,
            "task_type": "generation", 
            "timestamp": self.timestamp,
            "metrics": {
                "exact_match_ratio": exact_match,
                "avg_length_difference": avg_length_diff,
                "avg_reference_length": np.mean(ref_lengths),
                "avg_prediction_length": np.mean(pred_lengths)
            },
            "samples_count": len(references)
        }
        
        self.evaluation_results = results
        return results

    def plot_confusion_matrix(self, save_path: str = None):
        """
        ç»˜åˆ¶æ··æ·†çŸ©é˜µå›¾
        """
        if "confusion_matrix" in self.evaluation_results:
            cm = np.array(self.evaluation_results["confusion_matrix"])
            
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.title(f'Confusion Matrix for {self.model_name}')
            plt.ylabel('True Label')
            plt.xlabel('Predicted Label')
            
            if save_path:
                plt.savefig(save_path)
            plt.show()

    def save_report(self, filepath: str):
        """
        ä¿å­˜è¯„ä¼°æŠ¥å‘Š
        """
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.evaluation_results, f, indent=2, ensure_ascii=False)
        
        print(f"Evaluation report saved to {filepath}")

# Example usage
if __name__ == "__main__":
    evaluator = ModelEvaluator("SampleModel", "classification")
    
    # Example data
    y_true = [0, 1, 1, 0, 1, 0, 1, 1, 0, 0]
    y_pred = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1]
    
    results = evaluator.evaluate_classification(y_true, y_pred)
    print("Evaluation Results:", results)
    
    # Save report
    evaluator.save_report(f"evaluation_report_{evaluator.model_name}.json")
"""
    return template


def create_deployment_script_template() -> str:
    """
    åˆ›å»ºéƒ¨ç½²è„šæœ¬æ¨¡æ¿
    å‚è€ƒä¸Šä¸€ä»£å¹³å°çš„æ¨¡å‹éƒ¨ç½²åŠŸèƒ½
    """
    template = """
from flask import Flask, request, jsonify
import torch
import pickle
import pandas as pd
import numpy as np
import os
import logging
from typing import Dict, Any, Union

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelServer:
    def __init__(self, model_path: str, model_type: str = "torch"):
        """
        åˆå§‹åŒ–æ¨¡å‹æœåŠ¡
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            model_type: æ¨¡å‹ç±»å‹ ("torch", "sklearn", "transformers", "custom")
        """
        self.model_path = model_path
        self.model_type = model_type
        self.model = None
        self.tokenizer = None  # For transformer models
        self.load_model()
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            if self.model_type == "torch":
                self.model = torch.load(self.model_path)
                self.model.eval()
            elif self.model_type == "sklearn":
                with open(self.model_path, 'rb') as f:
                    self.model = pickle.load(f)
            elif self.model_type == "transformers":
                from transformers import AutoModel, AutoTokenizer
                self.model = AutoModel.from_pretrained(self.model_path)
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            else:
                # Custom model loading logic
                logger.warning(f"Unsupported model type: {self.model_type}. Using placeholder.")
                self.model = lambda x: {"prediction": "placeholder", "confidence": 0.5}
            
            logger.info(f"Model loaded successfully from {self.model_path}")
        except Exception as e:
            logger.error(f"Failed to load model: {str(e)}")
            raise e
    
    def predict(self, input_data: Union[Dict, List, str]) -> Dict[str, Any]:
        """
        æ‰§è¡Œé¢„æµ‹
        
        Args:
            input_data: è¾“å…¥æ•°æ®
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        try:
            if self.model_type == "torch":
                # Process input for PyTorch model
                tensor_input = torch.tensor(input_data) if not isinstance(input_data, torch.Tensor) else input_data
                with torch.no_grad():
                    prediction = self.model(tensor_input)
                    if isinstance(prediction, torch.Tensor):
                        prediction = prediction.numpy()
                
                result = {
                    "prediction": prediction.tolist() if hasattr(prediction, 'tolist') else prediction,
                    "model_type": self.model_type,
                    "success": True
                }
                
            elif self.model_type == "transformers":
                inputs = self.tokenizer(input_data, return_tensors="pt", padding=True, truncation=True)
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    predictions = torch.softmax(outputs.logits, dim=-1)
                    predicted_class = torch.argmax(predictions, dim=-1).item()
                    confidence = predictions[0][predicted_class].item()
                
                result = {
                    "prediction": predicted_class,
                    "confidence": confidence,
                    "model_type": self.model_type,
                    "success": True
                }
                
            else:  # sklearn or custom
                prediction = self.model.predict([input_data]) if hasattr(self.model, 'predict') else input_data
                result = {
                    "prediction": prediction[0] if isinstance(prediction, (list, np.ndarray)) else prediction,
                    "model_type": self.model_type,
                    "success": True
                }
            
            logger.info(f"Prediction completed successfully")
            return result
            
        except Exception as e:
            logger.error(f"Prediction failed: {str(e)}")
            return {
                "error": str(e),
                "success": False
            }

# Initialize Flask app
app = Flask(__name__)

# Global model server instance
model_server = None

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({"status": "healthy", "model_loaded": model_server is not None})

@app.route('/predict', methods=['POST'])
def predict():
    """é¢„æµ‹ç«¯ç‚¹"""
    global model_server
    
    if model_server is None:
        return jsonify({"error": "Model not loaded"}), 500
    
    try:
        data = request.get_json()
        input_data = data.get('input', {})
        
        result = model_server.predict(input_data)
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"Prediction request failed: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/reload', methods=['POST'])
def reload_model():
    """é‡æ–°åŠ è½½æ¨¡å‹"""
    global model_server
    
    try:
        data = request.get_json()
        model_path = data.get('model_path')
        model_type = data.get('model_type', 'torch')
        
        model_server = ModelServer(model_path, model_type)
        return jsonify({"status": "reloaded", "model_path": model_path})
        
    except Exception as e:
        logger.error(f"Model reload failed: {str(e)}")
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 3:
        print("Usage: python deployment_server.py <model_path> <model_type>")
        sys.exit(1)
    
    model_path = sys.argv[1]
    model_type = sys.argv[2]
    
    # Initialize model server
    model_server = ModelServer(model_path, model_type)
    
    # Run Flask app
    app.run(host='0.0.0.0', port=5000, debug=False)
"""
    return template


# æ³¨å†Œè¿™äº›æ¨¡æ¿ä¸ºVibecodingçš„ä»£ç ç”Ÿæˆæ¨¡æ¿
code_gen_templates = {
    "training_pipeline": create_training_pipeline_template,
    "evaluation_script": create_evaluation_script_template,
    "deployment_script": create_deployment_script_template
}
'''
    
    # åˆ›å»ºVibecodingæ¨¡æ¿æ–‡ä»¶
    vibecoding_dir = "vibecoding/templates"
    os.makedirs(vibecoding_dir, exist_ok=True)
    
    with open(f"{vibecoding_dir}/model_dev_templates.py", "w", encoding="utf-8") as f:
        f.write(code_templates)
    
    print("   âœ“ åˆ›å»ºæ¨¡å‹å¼€å‘ç›¸å…³ä»£ç æ¨¡æ¿")


def create_integration_examples(legacy_data: Dict[str, Any]):
    """åˆ›å»ºé›†æˆç¤ºä¾‹ï¼Œå±•ç¤ºä¸‰å¤§æ¨¡å—å¦‚ä½•ååŒå·¥ä½œ"""
    print("[ENHANCE] åˆ›å»ºé›†æˆç¤ºä¾‹...")
    
    integration_example = '''
"""
AI-Plat å¹³å°é›†æˆç¤ºä¾‹
å±•ç¤ºæœ¬ä½“è®ºã€æ™ºèƒ½ä½“å’ŒVibecodingä¸‰å¤§æ¨¡å—å¦‚ä½•ååŒå·¥ä½œ
åŸºäºä¸Šä¸€ä»£å¹³å°åŠŸèƒ½ç‚¹åˆ†æç»“æœ
"""

from ontology.ontology_manager import OntologyManager
from agents.skill_agent import SkillAgent
from agents.agent_orchestrator import AgentOrchestrator
from agents.skill_registry import global_skill_registry
from vibecoding.notebook_interface import VibecodingNotebookInterface
from vibecoding.code_generator import CodeGenerator
import asyncio
import uuid
from datetime import datetime


async def integrated_model_lifecycle_example():
    """
    é›†æˆç¤ºä¾‹ï¼šå®Œæ•´çš„æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†
    åŸºäºä¸Šä¸€ä»£å¹³å°çš„æ¨¡å‹ç®¡ç†ã€è®­ç»ƒã€è¯„ä¼°ã€æ¨ç†åŠŸèƒ½
    """
    print("="*60)
    print("ğŸ”„ å¼€å§‹æ‰§è¡Œé›†æˆæ¨¡å‹ç”Ÿå‘½å‘¨æœŸç¤ºä¾‹")
    print("="*60)
    
    # 1. ä½¿ç”¨æœ¬ä½“è®ºæ¨¡å—å®šä¹‰æ¨¡å‹èµ„äº§
    print("\\n1. ğŸ—ï¸ ä½¿ç”¨æœ¬ä½“è®ºæ¨¡å—å®šä¹‰æ¨¡å‹èµ„äº§")
    ontology_mgr = OntologyManager("./tmp_ontology_defs")
    
    # å®šä¹‰æ¨¡å‹ç±»å‹å’Œå±æ€§
    ontology_mgr.create_entity("LargeLanguageModel", "Class", "å¤§è¯­è¨€æ¨¡å‹")
    ontology_mgr.create_entity("VisionModel", "Class", "è§†è§‰æ¨¡å‹")
    ontology_mgr.create_entity("TrainingMethod", "Class", "è®­ç»ƒæ–¹æ³•")
    ontology_mgr.create_entity("FineTuning", "NamedIndividual", "å¾®è°ƒæ–¹æ³•")
    ontology_mgr.create_entity("usesTrainingMethod", "ObjectProperty", "ä½¿ç”¨è®­ç»ƒæ–¹æ³•")
    
    # åˆ›å»ºå…·ä½“æ¨¡å‹å®ä¾‹
    model_id = f"LLM-{uuid.uuid4().hex[:8]}"
    ontology_mgr.create_entity(model_id, "NamedIndividual", f"æ¨¡å‹å®ä¾‹: {model_id}")
    ontology_mgr.create_relationship(model_id, "rdf:type", "LargeLanguageModel")
    ontology_mgr.create_relationship(model_id, "usesTrainingMethod", "FineTuning")
    
    print(f"   âœ“ å®šä¹‰äº†æ¨¡å‹å®ä¾‹: {model_id}")
    
    # 2. ä½¿ç”¨æ™ºèƒ½ä½“æ¨¡å—æ‰§è¡Œæ¨¡å‹æ“ä½œ
    print("\\n2. ğŸ¤– ä½¿ç”¨æ™ºèƒ½ä½“æ¨¡å—æ‰§è¡Œæ¨¡å‹æ“ä½œ")
    
    # åˆ›å»ºæ¨¡å‹æ“ä½œä»£ç†
    model_agent = SkillAgent(
        name="ModelLifecycleAgent",
        description="è´Ÿè´£æ¨¡å‹å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†çš„æ™ºèƒ½ä½“",
        skills=[]  # ä¼šåœ¨åˆå§‹åŒ–åå¡«å……
    )
    await model_agent.initialize()
    
    # è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ç›¸å…³æŠ€èƒ½
    model_skills = []
    for skill_id in global_skill_registry.skills.keys():
        skill_meta = global_skill_registry.skills[skill_id].metadata
        if any(tag in ['training', 'evaluation', 'inference', 'ml', 'model'] for tag in skill_meta.tags):
            model_skills.append(skill_id)
    
    # ä¸ºä»£ç†åˆ†é…æŠ€èƒ½
    model_agent.skills = model_skills[:3]  # åˆ†é…å‰3ä¸ªæ¨¡å‹ç›¸å…³æŠ€èƒ½
    
    # æ‰§è¡Œè®­ç»ƒä»»åŠ¡
    if len(model_agent.skills) > 0:
        training_task_id = await model_agent.add_task(
            name="Train New Model",
            description="ä½¿ç”¨SFTæ–¹æ³•è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹",
            skill_id=model_agent.skills[0],  # å‡è®¾ç¬¬ä¸€ä¸ªæ˜¯è®­ç»ƒæŠ€èƒ½
            parameters={
                "model_type": "large_language_model",
                "training_method": "sft",
                "dataset_path": "/datasets/training_data.jsonl",
                "hyperparameters": {
                    "learning_rate": 5e-5,
                    "batch_size": 16,
                    "epochs": 3
                }
            }
        )
        print(f"   âœ“ æäº¤è®­ç»ƒä»»åŠ¡: {training_task_id}")
    
    # 3. ä½¿ç”¨Vibecodingæ¨¡å—ç”Ÿæˆåˆ†æä»£ç 
    print("\\n3. ğŸ’» ä½¿ç”¨Vibecodingæ¨¡å—ç”Ÿæˆåˆ†æä»£ç ")
    
    vibecoding_interface = VibecodingNotebookInterface()
    
    # åˆ›å»ºåˆ†æç¬”è®°æœ¬
    notebook_id = vibecoding_interface.create_notebook(
        name="Model Lifecycle Analysis",
        description="åˆ†ææ¨¡å‹ç”Ÿå‘½å‘¨æœŸå„é˜¶æ®µçš„æ€§èƒ½æŒ‡æ ‡"
    )
    
    # æ·»åŠ æ•°æ®å¤„ç†ä»£ç å•å…ƒ
    data_analysis_code = f"""
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# æ¨¡æ‹Ÿæ¨¡å‹ç”Ÿå‘½å‘¨æœŸæ•°æ®
phases = ['Training', 'Validation', 'Testing', 'Deployment']
durations = [2.5, 0.3, 0.2, 0.1]  # in hours
accuracies = [0.85, 0.82, 0.84, 0.83]

# Create dataframe
df = pd.DataFrame({{
    'Phase': phases,
    'Duration_Hours': durations,
    'Accuracy': accuracies
}})

print("æ¨¡å‹ç”Ÿå‘½å‘¨æœŸåˆ†æ:")
print(df)

# Visualization
fig, ax1 = plt.subplots(figsize=(10, 6))

color = 'tab:red'
ax1.set_xlabel('Phase')
ax1.set_ylabel('Duration (hours)', color=color)
bars = ax1.bar(phases, durations, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'], alpha=0.7)
ax1.tick_params(axis='y', labelcolor=color)

ax2 = ax1.twinx()
color = 'tab:blue'
ax2.set_ylabel('Accuracy', color=color)
line = ax2.plot(phases, accuracies, color=color, marker='o', linewidth=2, markersize=8)
ax2.tick_params(axis='y', labelcolor=color)

plt.title('Model Lifecycle Performance Dashboard')
plt.tight_layout()
plt.show()

print(f"\\næ¨¡å‹ç”Ÿå‘½å‘¨æœŸæ€»è€—æ—¶: {{sum(durations)}} å°æ—¶")
print(f"å¹³å‡å‡†ç¡®ç‡: {{np.mean(accuracies):.2f}}")
"""
    
    vibecoding_interface.add_cell(notebook_id, "code", data_analysis_code)
    
    # 4. æ‰§è¡Œç¬”è®°æœ¬
    print("\\n4. â–¶ï¸ æ‰§è¡Œåˆ†æç¬”è®°æœ¬")
    execution_result = await vibecoding_interface.execute_notebook(notebook_id)
    print(f"   âœ“ æ‰§è¡Œå®Œæˆ: {execution_result['successful_executions']}/{execution_result['executed_cells']} æˆåŠŸ")
    
    # 5. ä¿å­˜æœ¬ä½“å®šä¹‰
    print("\\n5. ğŸ’¾ ä¿å­˜æœ¬ä½“å®šä¹‰")
    ontology_mgr.save_ontology("model_lifecycle_demo")
    print("   âœ“ æœ¬ä½“å®šä¹‰å·²ä¿å­˜")
    
    print("\\n" + "="*60)
    print("âœ… é›†æˆæ¨¡å‹ç”Ÿå‘½å‘¨æœŸç¤ºä¾‹æ‰§è¡Œå®Œæˆ")
    print("="*60)
    
    return {
        "model_id": model_id,
        "training_task_id": training_task_id if 'training_task_id' in locals() else None,
        "notebook_execution": execution_result,
        "ontology_saved": True
    }


def demonstrate_advanced_features(legacy_data: Dict[str, Any]):
    """
    æ¼”ç¤ºé«˜çº§åŠŸèƒ½ï¼ŒåŸºäºä¸Šä¸€ä»£å¹³å°çš„å¤æ‚åŠŸèƒ½
    """
    print("\\n" + "="*60)
    print("ğŸš€ æ¼”ç¤ºé«˜çº§åŠŸèƒ½æ•´åˆ")
    print("="*60)
    
    # åˆ†æä¸Šä¸€ä»£å¹³å°çš„é«˜çº§åŠŸèƒ½
    advanced_features = []
    for feature in legacy_data['valuable_features']:
        feature_text = ' '.join([str(v) for v in feature.values()]).lower()
        if any(keyword in feature_text for keyword in ['pipeline', 'workflow', 'automated', 'orchestration', 'multi-model', 'ensemble']):
            advanced_features.append(feature)
    
    print(f"è¯†åˆ«åˆ° {len(advanced_features)} ä¸ªé«˜çº§åŠŸèƒ½æ¦‚å¿µ")
    
    # åˆ›å»ºé«˜çº§åŠŸèƒ½æ¼”ç¤ºä»£ç 
    advanced_demo_code = f"""
from agents.agent_orchestrator import AgentOrchestrator, WorkflowTask, TaskDependencyType
from agents.skill_agent import TaskPriority
import asyncio

async def demonstrate_advanced_workflow():
    print("å¼€å§‹æ¼”ç¤ºé«˜çº§å·¥ä½œæµåŠŸèƒ½...")
    
    # åˆ›å»ºç¼–æ’å™¨
    orchestrator = AgentOrchestrator()
    
    # è¿™é‡Œä¼šé›†æˆä¸Šä¸€ä»£å¹³å°çš„å¤æ‚åŠŸèƒ½æ¦‚å¿µ
    print("é«˜çº§åŠŸèƒ½æ¼”ç¤ºå·²å‡†å¤‡å°±ç»ª")
    print("- æ”¯æŒå¤æ‚å·¥ä½œæµç¼–æ’")
    print("- æ”¯æŒå¤šæ¨¡å‹ååŒå·¥ä½œ") 
    print("- æ”¯æŒè‡ªåŠ¨åŒ–ä»»åŠ¡è°ƒåº¦")
    print("- æ”¯æŒèµ„æºä¼˜åŒ–åˆ†é…")
    
    # åŸºäºåˆ†æçš„é«˜çº§åŠŸèƒ½æ¦‚å¿µåˆ›å»ºç¤ºä¾‹å·¥ä½œæµ
    print("\\nåŸºäºä¸Šä¸€ä»£å¹³å°åŠŸèƒ½åˆ†æï¼ŒAI-Platæ”¯æŒ:")
    for i, feature in enumerate(advanced_features[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
        print(f"  {i+1}. {feature.get('ä¸€çº§åŠŸèƒ½', 'N/A')}: {feature.get('åŠŸèƒ½æè¿°', '')[:100]}...")
    
    return True

# è¿è¡Œæ¼”ç¤º
# await demonstrate_advanced_workflow()
"""
    
    print(advanced_demo_code)
    
    print("\\nâœ… é«˜çº§åŠŸèƒ½æ¼”ç¤ºåˆ›å»ºå®Œæˆ")


if __name__ == "__main__":
    # åŠ è½½ä¸Šä¸€ä»£åŠŸèƒ½åˆ†æç»“æœ
    print("[LOAD] åŠ è½½ä¸Šä¸€ä»£åŠŸèƒ½åˆ†æç»“æœ...")
    try:
        legacy_data = load_legacy_analysis()
        print("   âœ“ åˆ†æç»“æœåŠ è½½æˆåŠŸ")
    except FileNotFoundError:
        print("   âš  æœªæ‰¾åˆ°åˆ†æç»“æœæ–‡ä»¶ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        legacy_data = {
            'valuable_features': [
                {'ä¸€çº§åŠŸèƒ½': 'æ¨¡å‹ç®¡ç†', 'åŠŸèƒ½æè¿°': 'æ”¯æŒæ¨¡å‹çš„å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†'},
                {'ä¸€çº§åŠŸèƒ½': 'æ¨¡å‹è®­ç»ƒ', 'åŠŸèƒ½æè¿°': 'æ”¯æŒå¤šç§è®­ç»ƒæ–¹æ³•'},
                {'ä¸€çº§åŠŸèƒ½': 'æ¨¡å‹è¯„ä¼°', 'åŠŸèƒ½æè¿°': 'æ”¯æŒè‡ªåŠ¨åŒ–è¯„ä¼°'},
                {'ä¸€çº§åŠŸèƒ½': 'æ¨¡å‹æ¨ç†', 'åŠŸèƒ½æè¿°': 'æ”¯æŒé«˜æ€§èƒ½æ¨ç†æœåŠ¡'}
            ]
        }
    
    # å¢å¼ºå„ä¸ªæ¨¡å—
    enhance_ontology_module(legacy_data)
    enhance_agent_module(legacy_data) 
    enhance_vibecoding_module(legacy_data)
    
    # åˆ›å»ºé›†æˆç¤ºä¾‹
    create_integration_examples(legacy_data)
    
    # æ¼”ç¤ºé«˜çº§åŠŸèƒ½
    demonstrate_advanced_features(legacy_data)
    
    print("\\n" + "="*70)
    print("ğŸ‰ AI-Platå¹³å°å·²åŸºäºä¸Šä¸€ä»£åŠŸèƒ½ç‚¹å®Œæˆå¢å¼º!")
    print("   âœ“ æœ¬ä½“è®ºæ¨¡å—: å¢å¼ºäº†æ¨¡å‹èµ„äº§ç®¡ç†èƒ½åŠ›")
    print("   âœ“ æ™ºèƒ½ä½“æ¨¡å—: å¢åŠ äº†æ¨¡å‹è®­ç»ƒ/è¯„ä¼°/æ¨ç†æŠ€èƒ½") 
    print("   âœ“ Vibecodingæ¨¡å—: æ·»åŠ äº†å¼€å‘æ¨¡æ¿å’Œæœ€ä½³å®è·µ")
    print("   âœ“ é›†æˆç¤ºä¾‹: å±•ç¤ºäº†ä¸‰å¤§æ¨¡å—ååŒå·¥ä½œ")
    print("="*70)
'''
    
    # åˆ›å»ºé›†æˆç¤ºä¾‹æ–‡ä»¶
    examples_dir = "examples"
    os.makedirs(examples_dir, exist_ok=True)
    
    with open(f"{examples_dir}/integration_example.py", "w", encoding="utf-8") as f:
        f.write(integration_example)
    
    print("   âœ“ åˆ›å»ºé›†æˆç¤ºä¾‹")


def main():
    """ä¸»å‡½æ•°"""
    print("[ENHANCE] å¼€å§‹åŸºäºä¸Šä¸€ä»£åŠŸèƒ½ç‚¹å¢å¼ºAI-Platå¹³å°")
    
    # åŠ è½½åˆ†æç»“æœ
    try:
        legacy_data = load_legacy_analysis()
        print("[OK] åŠ è½½ä¸Šä¸€ä»£åŠŸèƒ½åˆ†æç»“æœ")
    except FileNotFoundError:
        print("[WARN] æœªæ‰¾åˆ°åˆ†æç»“æœæ–‡ä»¶ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        legacy_data = {
            'valuable_features': [
                {'ä¸€çº§åŠŸèƒ½': 'æ¨¡å‹ç®¡ç†', 'åŠŸèƒ½æè¿°': 'æ”¯æŒæ¨¡å‹çš„å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†'},
                {'ä¸€çº§åŠŸèƒ½': 'æ¨¡å‹è®­ç»ƒ', 'åŠŸèƒ½æè¿°': 'æ”¯æŒå¤šç§è®­ç»ƒæ–¹æ³•'},
                {'ä¸€çº§åŠŸèƒ½': 'æ¨¡å‹è¯„ä¼°', 'åŠŸèƒ½æè¿°': 'æ”¯æŒè‡ªåŠ¨åŒ–è¯„ä¼°'},
                {'ä¸€çº§åŠŸèƒ½': 'æ¨¡å‹æ¨ç†', 'åŠŸèƒ½æè¿°': 'æ”¯æŒé«˜æ€§èƒ½æ¨ç†æœåŠ¡'}
            ]
        }
    
    # æ‰§è¡Œå„é¡¹å¢å¼º
    enhance_ontology_module(legacy_data)
    enhance_agent_module(legacy_data)
    enhance_vibecoding_module(legacy_data)
    create_integration_examples(legacy_data)
    
    print(f"\n{'='*60}")
    print("[SUCCESS] AI-Platå¹³å°å¢å¼ºå®Œæˆ!")
    print("   å¹³å°ç°å·²é›†æˆä¸Šä¸€ä»£å¹³å°çš„æ ¸å¿ƒåŠŸèƒ½ä¼˜ç‚¹ï¼ŒåŒ…æ‹¬:")
    print("   - å¼ºå¤§çš„æ¨¡å‹èµ„äº§ç®¡ç†èƒ½åŠ› (åŸºäºæœ¬ä½“è®º)")
    print("   - çµæ´»çš„æ¨¡å‹è®­ç»ƒ/è¯„ä¼°/æ¨ç†æ™ºèƒ½ä½“")
    print("   - ä¸°å¯Œçš„ä»£ç ç”Ÿæˆå’Œå¼€å‘æ¨¡æ¿")
    print("   - å®Œæ•´çš„æ¨¡å‹ç”Ÿå‘½å‘¨æœŸé›†æˆç¤ºä¾‹")
    print("="*60)


if __name__ == "__main__":
    main()